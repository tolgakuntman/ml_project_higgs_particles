{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a76afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path.cwd().parent.joinpath('src').resolve()))\n",
    "from utils import *\n",
    "import copy\n",
    "import math\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca1f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data(\"../data/samples/higgs_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da2ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five elements in X_train are:\n",
      "          f1        f2        f3        f4        f5        f6        f7  \\\n",
      "0  0.719958  0.785939  0.981966  0.553060  1.383569  0.925050  1.853685   \n",
      "1  0.673473  0.110004 -1.289741  1.493732  1.224188  1.020413  0.265356   \n",
      "2  1.228540 -0.038039 -0.914100  0.579205  0.240356  0.813564  0.720862   \n",
      "3  2.154200  0.142145 -1.209841  1.915257 -1.589158  2.047788 -1.414074   \n",
      "4  1.502871 -0.042909 -0.124530  2.707755 -0.449995  0.880712 -0.276300   \n",
      "\n",
      "         f8        f9       f10  ...       f19       f20       f21       f22  \\\n",
      "0 -1.074324  0.000000  0.905498  ...  0.892326  0.797247  3.101961  1.014163   \n",
      "1 -0.701229  0.000000  0.920984  ... -0.327760 -1.343154  3.101961  1.564466   \n",
      "2  1.177100  1.086538  1.150884  ...  1.026410  0.569178  1.550981  1.401413   \n",
      "3  0.177561  0.000000  0.784379  ... -0.152867  1.314980  0.000000  0.916103   \n",
      "4  1.111683  0.000000  0.771159  ... -0.712525  1.622956  0.000000  0.951986   \n",
      "\n",
      "        f23       f24       f25       f26       f27       f28  \n",
      "0  0.921529  0.983500  0.871629  0.749826  0.835315  0.749713  \n",
      "1  0.943373  1.067275  0.642923  0.323242  0.777785  0.780921  \n",
      "2  1.105798  1.008392  1.032063  0.415143  0.969382  0.839391  \n",
      "3  0.799980  0.983402  1.073133  0.487141  1.132719  0.986468  \n",
      "4  0.936494  0.987740  0.866998  0.757818  0.852293  0.997009  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First five elements in X_train are:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4a8dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five elements in y_train are:\n",
      " [1 0 1 1 0]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First five elements in y_train are:\\n\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187f210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (400000, 28)\n",
      "The shape of y_train is: (400000,)\n",
      "We have m = 400000 training examples\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is: ' + str(X_train.shape))\n",
    "print ('The shape of y_train is: ' + str(y_train.shape))\n",
    "print ('We have m = %d training examples' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c580b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a88ccc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_=0.0):\n",
    "    m = X.shape[0]\n",
    "    z = X.dot(w) + b           # shape (m,)\n",
    "    p = sigmoid(z)             # shape (m,)\n",
    "    # clip to avoid log(0)\n",
    "    eps = 1e-12\n",
    "    cost = - (y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps)).mean()\n",
    "    # add regularization if desired:\n",
    "    if lambda_:\n",
    "        cost += (lambda_ / (2*m)) * np.sum(w**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2637c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at initial w (zeros): 0.693\n"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd045fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=0.0):\n",
    "    m = X.shape[0]\n",
    "    z = X.dot(w) + b\n",
    "    p = sigmoid(z)\n",
    "    error = p - y               # shape (m,)\n",
    "    dj_dw = (X.T.dot(error) / m) + (lambda_ / m) * w\n",
    "    dj_db = error.mean()\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7dbb07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w (zeros):-0.02879\n",
      "dj_dw at initial w (zeros):[-0.014671717771850527, 0.00042985919293016196, -0.001118724101784046, 0.00202225888157438, -0.000175184210143032, -0.042020431100856515, -0.0003710050517620857, 0.0005153622108769695, -0.023820989152491093, -0.0345640292387642, -0.0007117815831749067, 0.001275515634009098, -0.0032863165107369415, -0.03209177182596177, -0.001303456493557278, -0.00040129368487921963, -0.014346503648757935, -0.03738590491063893, -0.00042448005668884435, 0.0006411599186694729, -0.037737298844009635, -0.03427435940969736, -0.0342536618277058, -0.03123258922241628, -0.02289227080712095, 0.011407724410700612, -0.017615057053864003, -0.008397993431910873]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient with w initialized to zeroes\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_db at initial w (zeros):{dj_db}' )\n",
    "print(f'dj_dw at initial w (zeros):{dj_dw.tolist()}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c974d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (array_like Shape (m, n)\n",
    "      y :    (array_like Shape (m,))\n",
    "      w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)                 Initial value of parameter of the model\n",
    "      cost_function:                  function to compute cost\n",
    "      alpha : (float)                 Learning rate\n",
    "      num_iters : (int)               number of iterations to run gradient descent\n",
    "      lambda_ (scalar, float)         regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa8a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.69   \n",
      "Iteration 1000: Cost     0.68   \n",
      "Iteration 1000: Cost     0.68   \n",
      "Iteration 2000: Cost     0.67   \n",
      "Iteration 2000: Cost     0.67   \n",
      "Iteration 3000: Cost     0.67   \n",
      "Iteration 3000: Cost     0.67   \n",
      "Iteration 4000: Cost     0.66   \n",
      "Iteration 4000: Cost     0.66   \n",
      "Iteration 5000: Cost     0.66   \n",
      "Iteration 5000: Cost     0.66   \n",
      "Iteration 6000: Cost     0.66   \n",
      "Iteration 6000: Cost     0.66   \n",
      "Iteration 7000: Cost     0.66   \n",
      "Iteration 7000: Cost     0.66   \n",
      "Iteration 8000: Cost     0.65   \n",
      "Iteration 8000: Cost     0.65   \n",
      "Iteration 9000: Cost     0.65   \n",
      "Iteration 9000: Cost     0.65   \n",
      "Iteration 9999: Cost     0.65   \n",
      "w shape: (28,)\n",
      "b: 0.2215982435920402\n",
      "Last cost: 0.6514710420448836\n",
      "Iteration 9999: Cost     0.65   \n",
      "w shape: (28,)\n",
      "b: 0.2215982435920402\n",
      "Last cost: 0.6514710420448836\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = np.random.randn(n) * 0.01  # shape (n,) small random values\n",
    "initial_b = 0.0\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.005\n",
    "\n",
    "# Run gradient descent (batch)\n",
    "w, b, J_history, _ = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                           compute_cost, compute_gradient, alpha, iterations, 0)\n",
    "\n",
    "print('w shape:', w.shape)\n",
    "print('b:', b)\n",
    "print('Last cost:', J_history[-1] if len(J_history) > 0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7a90332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of J_history: 10000\n",
      "First 10 costs: [np.float64(0.693932819724718), np.float64(0.6938378641335086), np.float64(0.6937463780701792), np.float64(0.6936582030857349), np.float64(0.69357318800832), np.float64(0.6934911886112227), np.float64(0.6934120672957609), np.float64(0.6933356927884076), np.float64(0.6932619398515436), np.float64(0.6931906890072443)]\n",
      "Current dj_db (scalar): -0.002110130677671557\n",
      "Norm of dj_dw (L2): 0.013215621673833323\n",
      "Top 10 features by std (index, std):\n",
      "20 1.3997997954168464\n",
      "16 1.1940273109730548\n",
      "12 1.049895860655212\n",
      "8 1.0275673890706865\n",
      "1 1.0091470710892596\n",
      "14 1.0088198543150915\n",
      "6 1.0086747337804196\n",
      "18 1.007825876768817\n",
      "10 1.00773446777511\n",
      "2 1.0068005887708906\n",
      "Could not compute feature stats: TypeError(\"cannot convert the series to <class 'float'>\")\n",
      "Label distribution (val:count, fraction):\n",
      "0 188484 0.47121\n",
      "1 211516 0.52879\n",
      "Using subset size=50000 for scaled experiment\n",
      "Running gradient descent on scaled subset: alpha=0.05, iters=2000\n",
      "Iteration    0: Cost     0.69   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f_/ps1v2btn63d27h0glvzpqshm0000gn/T/ipykernel_66737/3165227829.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(int(i), float(feature_stds[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  200: Cost     0.65   \n",
      "Iteration  400: Cost     0.65   \n",
      "Iteration  400: Cost     0.65   \n",
      "Iteration  600: Cost     0.64   \n",
      "Iteration  600: Cost     0.64   \n",
      "Iteration  800: Cost     0.64   \n",
      "Iteration  800: Cost     0.64   \n",
      "Iteration 1000: Cost     0.64   \n",
      "Iteration 1000: Cost     0.64   \n",
      "Iteration 1200: Cost     0.64   \n",
      "Iteration 1200: Cost     0.64   \n",
      "Iteration 1400: Cost     0.64   \n",
      "Iteration 1400: Cost     0.64   \n",
      "Iteration 1600: Cost     0.64   \n",
      "Iteration 1600: Cost     0.64   \n",
      "Iteration 1800: Cost     0.64   \n",
      "Iteration 1800: Cost     0.64   \n",
      "Iteration 1999: Cost     0.64   \n",
      "Scaled subset initial cost: 0.6922998307981906 final cost: 0.6398136646451976 reduction: 0.05248616615299295\n",
      "Iteration 1999: Cost     0.64   \n",
      "Scaled subset initial cost: 0.6922998307981906 final cost: 0.6398136646451976 reduction: 0.05248616615299295\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics: check J_history, gradient norms, feature scales and label balance\n",
    "try:\n",
    "    print('Length of J_history:', len(J_history))\n",
    "    print('First 10 costs:', J_history[:10])\n",
    "except NameError:\n",
    "    print('J_history not found in this scope. Run the gradient-descent cell first.')\n",
    "\n",
    "# Current parameters (if available) and gradient norms\n",
    "try:\n",
    "    dj_db_curr, dj_dw_curr = compute_gradient(X_train, y_train, w, b)\n",
    "    print('Current dj_db (scalar):', float(dj_db_curr))\n",
    "    print('Norm of dj_dw (L2):', float(np.linalg.norm(dj_dw_curr)))\n",
    "except Exception as e:\n",
    "    print('Could not compute current gradient:', repr(e))\n",
    "\n",
    "# Feature scale summary (top 10 largest stds)\n",
    "try:\n",
    "    feature_means = X_train.mean(axis=0)\n",
    "    feature_stds = X_train.std(axis=0)\n",
    "    top_idx = np.argsort(feature_stds)[-10:][::-1]\n",
    "    print('Top 10 features by std (index, std):')\n",
    "    for i in top_idx:\n",
    "        print(int(i), float(feature_stds[i]))\n",
    "    print('Max absolute value in X_train:', float(np.abs(X_train).max()))\n",
    "except Exception as e:\n",
    "    print('Could not compute feature stats:', repr(e))\n",
    "\n",
    "# Label distribution\n",
    "try:\n",
    "    vals, counts = np.unique(y_train, return_counts=True)\n",
    "    print('Label distribution (val:count, fraction):')\n",
    "    for v,c in zip(vals, counts):\n",
    "        print(int(v), c, c/len(y_train))\n",
    "except Exception as e:\n",
    "    print('Could not compute label distribution:', repr(e))\n",
    "\n",
    "# Quick experiment: standardize features on a subset and re-run gradient descent to compare convergence speed\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    subset = 50000 if len(y_train) > 50000 else len(y_train)\n",
    "    print(f'Using subset size={subset} for scaled experiment')\n",
    "    X_sub = X_train[:subset].astype(float)\n",
    "    y_sub = y_train[:subset]\n",
    "    scaler = StandardScaler()\n",
    "    X_sub_s = scaler.fit_transform(X_sub)\n",
    "    # initialize small weights\n",
    "    w0 = np.zeros(n)\n",
    "    b0 = 0.0\n",
    "    alpha2 = 0.05\n",
    "    iters2 = 2000\n",
    "    print(f'Running gradient descent on scaled subset: alpha={alpha2}, iters={iters2}')\n",
    "    w2, b2, J2, _ = gradient_descent(X_sub_s, y_sub, w0, b0, compute_cost, compute_gradient, alpha2, iters2, 0)\n",
    "    print('Scaled subset initial cost:', J2[0], 'final cost:', J2[-1], 'reduction:', J2[0]-J2[-1])\n",
    "except Exception as e:\n",
    "    print('Scaled experiment skipped or failed (sklearn may be missing):', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec0cd788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "    X : (ndarray Shape (m, n))\n",
    "    w : (array_like Shape (n,))      Parameters of the model\n",
    "    b : (scalar, float)              Parameter of the model\n",
    "\n",
    "    Returns:\n",
    "    p: (ndarray (m,1))\n",
    "        The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### BEGIN CODE HERE\n",
    "    fw = sigmoid(np.dot(X, w) + b)\n",
    "    p = (fw >= 0.5).astype(int)\n",
    "    \n",
    "    \n",
    "    ### END CODE\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d57008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_data(\"../data/samples/higgs_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72df2e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 61.958000\n"
     ]
    }
   ],
   "source": [
    "p = predict(X_test, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625066d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing evaluation...\n",
      "Loaded validation set: (50000, 28) (50000,)\n",
      "-- Train --\n",
      "Accuracy: 0.6184, Precision: 0.6192, Recall: 0.7230, F1: 0.6671\n",
      "ROC AUC: 0.6598\n",
      "Confusion matrix:\n",
      "[[ 94430  94054]\n",
      " [ 58593 152923]]\n",
      "-- Train --\n",
      "Accuracy: 0.6184, Precision: 0.6192, Recall: 0.7230, F1: 0.6671\n",
      "ROC AUC: 0.6598\n",
      "Confusion matrix:\n",
      "[[ 94430  94054]\n",
      " [ 58593 152923]]\n",
      "-- Test --\n",
      "Accuracy: 0.6196, Precision: 0.6197, Recall: 0.7260, F1: 0.6687\n",
      "ROC AUC: 0.6606\n",
      "Confusion matrix:\n",
      "[[11783 11778]\n",
      " [ 7243 19196]]\n",
      "-- Validation --\n",
      "Accuracy: 0.6179, Precision: 0.6193, Recall: 0.7202, F1: 0.6659\n",
      "ROC AUC: 0.6595\n",
      "Confusion matrix:\n",
      "[[11852 11708]\n",
      " [ 7397 19043]]\n",
      "Fitting sklearn LogisticRegression on subset=100000 (this may take a moment)\n",
      "-- Test --\n",
      "Accuracy: 0.6196, Precision: 0.6197, Recall: 0.7260, F1: 0.6687\n",
      "ROC AUC: 0.6606\n",
      "Confusion matrix:\n",
      "[[11783 11778]\n",
      " [ 7243 19196]]\n",
      "-- Validation --\n",
      "Accuracy: 0.6179, Precision: 0.6193, Recall: 0.7202, F1: 0.6659\n",
      "ROC AUC: 0.6595\n",
      "Confusion matrix:\n",
      "[[11852 11708]\n",
      " [ 7397 19043]]\n",
      "Fitting sklearn LogisticRegression on subset=100000 (this may take a moment)\n",
      "-- sklearn LogisticRegression (subset test) --\n",
      "Accuracy: 0.6412, Precision: 0.6387, Recall: 0.7402, F1: 0.6857\n",
      "ROC AUC: 0.6842\n",
      "Confusion matrix:\n",
      "[[12489 11072]\n",
      " [ 6870 19569]]\n",
      "Evaluation cell finished. Recommendations: scale features, try sklearn baseline or SGDClassifier, tune learning rate, and use minibatches or optimized solvers for large data.\n",
      "-- sklearn LogisticRegression (subset test) --\n",
      "Accuracy: 0.6412, Precision: 0.6387, Recall: 0.7402, F1: 0.6857\n",
      "ROC AUC: 0.6842\n",
      "Confusion matrix:\n",
      "[[12489 11072]\n",
      " [ 6870 19569]]\n",
      "Evaluation cell finished. Recommendations: scale features, try sklearn baseline or SGDClassifier, tune learning rate, and use minibatches or optimized solvers for large data.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation: compute common metrics and run a small sklearn baseline on a subset\n",
    "print('Preparing evaluation...')\n",
    "# load validation set if present\n",
    "try:\n",
    "    X_val, y_val = load_data('../data/samples/higgs_val.csv')\n",
    "    print('Loaded validation set:', X_val.shape, y_val.shape)\n",
    "except Exception as e:\n",
    "    X_val, y_val = None, None\n",
    "    print('Validation set not available or failed to load:', repr(e))\n",
    "\n",
    "# Utility: safe metrics display\n",
    "def print_metrics(name, y_true, y_pred, y_prob=None):\n",
    "    try:\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "    except Exception as e:\n",
    "        print('sklearn.metrics unavailable:', repr(e))\n",
    "        print(name, 'accuracy (fallback):', float((y_true==y_pred).mean()))\n",
    "        return\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    print(f'-- {name} --')\n",
    "    print(f'Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}')\n",
    "    try:\n",
    "        if y_prob is not None:\n",
    "            auc = roc_auc_score(y_true, y_prob)\n",
    "            print(f'ROC AUC: {auc:.4f}')\n",
    "    except Exception as e:\n",
    "        print('ROC AUC failed:', repr(e))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Predictions and metrics for train/val/test (use predict/probabilities)\n",
    "try:\n",
    "    # Ensure w,b exist\n",
    "    _ = w\n",
    "    _ = b\n",
    "    p_train = predict(X_train, w, b)\n",
    "    probs_train = sigmoid(X_train.dot(w) + b)\n",
    "    print_metrics('Train', y_train, p_train, probs_train)\n",
    "except Exception as e:\n",
    "    print('Train evaluation failed:', repr(e))\n",
    "\n",
    "try:\n",
    "    X_test, y_test = load_data('../data/samples/higgs_test.csv')\n",
    "    p_test = predict(X_test, w, b)\n",
    "    probs_test = sigmoid(X_test.dot(w) + b)\n",
    "    print_metrics('Test', y_test, p_test, probs_test)\n",
    "except Exception as e:\n",
    "    print('Test evaluation failed:', repr(e))\n",
    "\n",
    "if X_val is not None and y_val is not None:\n",
    "    try:\n",
    "        p_val = predict(X_val, w, b)\n",
    "        probs_val = sigmoid(X_val.dot(w) + b)\n",
    "        print_metrics('Validation', y_val, p_val, probs_val)\n",
    "    except Exception as e:\n",
    "        print('Validation evaluation failed:', repr(e))\n",
    "\n",
    "# Baseline: sklearn LogisticRegression on a subset (standardized)\n",
    "try:\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    subset = min(100000, len(y_train))\n",
    "    print(f'Fitting sklearn LogisticRegression on subset={subset} (this may take a moment)')\n",
    "    X_sub = X_train[:subset].astype(float)\n",
    "    y_sub = y_train[:subset]\n",
    "    scaler = StandardScaler().fit(X_sub)\n",
    "    X_sub_s = scaler.transform(X_sub)\n",
    "    clf = LogisticRegression(solver='saga', max_iter=200, n_jobs=-1, C=1.0)\n",
    "    clf.fit(X_sub_s, y_sub)\n",
    "    # evaluate on a scaled test subset\n",
    "    X_test_s = scaler.transform(X_test[:subset].astype(float)) if 'X_test' in globals() else None\n",
    "    y_test_sub = y_test[:subset] if 'y_test' in globals() else None\n",
    "    if X_test_s is not None:\n",
    "        preds = clf.predict(X_test_s)\n",
    "        probs = clf.predict_proba(X_test_s)[:,1]\n",
    "        print_metrics('sklearn LogisticRegression (subset test)', y_test_sub, preds, probs)\n",
    "    else:\n",
    "        print('sklearn baseline fitted but no test data available to evaluate on subset')\n",
    "except Exception as e:\n",
    "    print('sklearn baseline skipped or failed:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957045b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
